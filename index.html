
<!DOCTYPE HTML>
<html lang="en">
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-X8CY1YZ5XC"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-X8CY1YZ5XC');
  </script>

  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Chien-Yi Wang</title>
  
  <meta name="author" content="Chien-Yi Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/cover_photo.png">
</head>

<body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr style="padding:0px">
          <td style="padding:0px">
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                    <p style="text-align:center">
                        <name>Chien-Yi Wang</name>
                    </p>
                    <p>
                        I am a Senior Research Scientist at <a href="https://www.nvidia.com/en-us/research/">NVIDIA Research</a>. Previously, I worked as a Senior Research SDE at <a href="https://news.microsoft.com/apac/2018/01/12/microsoft-launches-artificial-intelligence-research-hub-taiwan/">Microsoft AI R&D Center</a> in Taiwan. I had 8+ years of experience specializing in computer vision research, deep learning-based model optimization, and machine learning service integration. My research focus is mainly on cross-modality representation learning, face modeling, and 2D/3D scene understanding. Interested in revolutionizing a machine learning system from the bottom‑up, devising better problem‑solving methods for challenging tasks, and learning new technologies and tools if the need arises.
                    </p>
                    <p>
                        I received my M.S. degree in <a href="https://minghsiehece.usc.edu/">Electrical Engineering</a> from <a href="https://www.usc.edu/">University of Southern California (USC)</a> in 2016 and B.S. degree in <a href="https://web.ee.ntu.edu.tw/">Electrical Engineering</a> from <a href="https://www.ntu.edu.tw/">National Taiwan University (NTU)</a> in 2013, respectively.  
                    </p>
                    
                    <p style="text-align:center">
                    <a href="mailto:chienyiwang0922@gmail.com">Email</a> &nbsp;/&nbsp;
                    <a href="CV_Chien_Yi_Wang_1003.pdf">CV</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?user=05LW2DcAAAAJ">Google Scholar</a> &nbsp;/&nbsp;
                    <a href="https://www.linkedin.com/in/chienyiwang"> LinkedIn </a>&nbsp;/&nbsp;
                    <a href="https://twitter.com/chienyi_wang">Twitter </a>&nbsp;/&nbsp;
                    <a href="https://github.com/chienyiwang">GitHub</a>
                    </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                    <img style="width:100%;max-width:100%" alt="profile photo" src="images/cover_photo.png">
                </td>
              </tr>
            </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <ul>
                <li> [Jan. 2023] One paper is accepted at <a href="https://tbiom.ieee-biometrics.org/">TBIOM</a>.
                <li> [Nov. 2022] Our paper "MixFairFace" is accepted at <a href="https://aaai.org/Conferences/AAAI-23/">AAAI 2023</a>.
                <li> [Oct. 2022] One paper is accepted at <a href="https://fg2023.ieee-biometrics.org/">FG 2023</a>. 
                <li> [Oct. 2022] I joined <a href="https://www.nvidia.com/en-us/research/">NVIDIA Research</a> as a Research Scientist.
                <li> [Jun. 2022] I am selected as an <a href="https://cvpr2022.thecvf.com/outstanding-reviewers">outstanding reviewer for CVPR 2022</a>!
                <li> [Mar. 2022] Two papers "PatchNet" and "LaFR" are accepted at <a href="https://cvpr2022.thecvf.com/">CVPR 2022</a>.
                <li> [Dec. 2021] Our paper "FedFR" is accepted at <a href="https://aaai.org/Conferences/AAAI-22/">AAAI 2022</a> as Oral!
                <li> [Oct. 2021] One paper is accepted at <a href="https://wacv2022.thecvf.com/home">WACV 2022</a>.
                <li> [Sep. 2021] One paper is accepted at <a href="http://iab-rubric.org/fg2021/index.html">FG 2021</a>.
               
              </ul>
            </td>
          </tr>
        </tbody></table>
            
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in computer vision and machine learning.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
        
        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/tbiom2023.png" alt="blind-date" width="160" height="160">
                </td>
            <td width="75%" valign="middle">
                <papertitle>RGB-D Face Recognition with Identity-Style Disentanglement and Depth Augmentation</papertitle>
                
                <br>
                <a href="mailto:s108062802@m108.nthu.edu.tw">Meng-Tzu Chiu</a>,
                <a href="mailto:s107062523@m107.nthu.edu.tw">Hsun-Ying Cheng</a>,
                <strong>Chien-Yi Wang</strong>, 
                <a href="http://www.cs.nthu.edu.tw/~lai/">Shang-Hong Lai</a>
                <br>
                <em>IEEE Transactions on Biometrics, Behavior, and Identity Science (TBIOM)</em>, 2023 &nbsp
                <br>
                <a href="https://ieeexplore.ieee.org/document/10011574">paper</a>
                <p></p>
                <p>
                  We propose to augment facial segmentation and depth maps to assist the RGB-D face recognition task. With the multi-modal augmentation and identity-style disentanglement, the proposed RGB-D recognition model could achieve superior performance on several benchmarks.
                </p>
            </td>
        </tr>
        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/mixfairface.png" alt="blind-date" width="160" height="160">
                </td>
            <td width="75%" valign="middle">
                <papertitle>MixFairFace: Towards Ultimate Fairness via MixFair Adapter in Face Recognition</papertitle>
                
                <br>
                <a href="https://fuenwang.ml/">Fu-En Wang</a>,
                <strong>Chien-Yi Wang</strong>,
                <a href="https://aliensunmin.github.io/">Min Sun</a>,
                <a href="http://www.cs.nthu.edu.tw/~lai/">Shang-Hong Lai</a>
                <br>
                <em>37th AAAI Conference on Artificial Intelligence (AAAI)</em>, 2023 &nbsp
                <br>
                <a href="https://arxiv.org/abs/2211.15181">arXiv</a>
                /
                <a href="https://github.com/fuenwang/MixFairFace">code</a>
                <p></p>
                <p>
                  We propose the MixFair Adapter to determine and reduce the identity bias of training samples. Besides, in order to push for ultimate fairness in face recognition, we propose a new evaluation protocol to fairly evaluate the fairness performance of different approaches.
                </p>
            </td>
        </tr>
        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/meta-fas.png" alt="blind-date" width="160" height="160">
                </td>
            <td width="75%" valign="middle">
                <papertitle>Generalized Face Anti-Spoofing via Multi-Task Learning and One-Side Meta Triplet Loss</papertitle>
                
                <br>
                <a href="https://www.linkedin.com/in/chu-chun-chuang-9b38a120a/">Chu-Chun Chuang</a>,
                <strong>Chien-Yi Wang</strong>,
                <a href="http://www.cs.nthu.edu.tw/~lai/">Shang-Hong Lai</a>
                <br>
                <em>IEEE International Conference on Automatic Face and Gesture Recognition (FG)</em>, 2023 &nbsp
                <br>
                <a href="https://arxiv.org/abs/2211.15955">arXiv</a>
                <p></p>
                <p>
                  We introduce a multi-task meta-learning framework for learning more generalized features for face anti-spoofing.
                </p>
            </td>
        </tr>
        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/patchnet.png" alt="blind-date" width="160" height="160">
                </td>
            <td width="75%" valign="middle">
                <papertitle>PatchNet: A Simple Face Anti-Spoofing Framework via Fine-Grained Patch Recognition</papertitle>
                
                <br>
                <strong>Chien-Yi Wang</strong>,
                <a href="https://jonlu0602.github.io/">Yu-Ding Lu</a>,
                <a href="https://tw.linkedin.com/in/shang-ta-yang-852429133">Shang-Ta Yang</a>,
                <a href="http://www.cs.nthu.edu.tw/~lai/">Shang-Hong Lai</a>
                <br>
                <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022 &nbsp
                <br>
                <a href="https://arxiv.org/abs/2203.14325">arXiv</a>
                /
                <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_PatchNet_A_Simple_Face_Anti-Spoofing_Framework_via_Fine-Grained_Patch_Recognition_CVPR_2022_paper.pdf">paper</a>
                /
                <a href="https://openaccess.thecvf.com/content/CVPR2022/supplemental/Wang_PatchNet_A_Simple_CVPR_2022_supplemental.pdf">supp</a>
                /
                <a href="https://youtu.be/pU1knahmees">video</a>
                <p></p>
                <p>
                  We propose PatchNet which reformulates face anti-spoofing as a fine-grained patch-type recognition problem.
                </p>
            </td>
        </tr>
        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/lafr.png" alt="blind-date" width="160" height="160">
                </td>
            <td width="75%" valign="middle">
                <papertitle>Local-Adaptive Face Recognition via Graph-based Meta-Clustering and Regularized Adaptation</papertitle>
                
                <br>
                <strong>Chien-Yi Wang*</strong>,
                <a href="https://www.linkedin.com/in/wenbin-zhu-4152148a/">Wenbin Zhu*</a>,
                <a href="https://scholar.google.com/citations?user=Lwbsw_QAAAAJ&hl=zh-TW">Kuan Lun Tseng</a>,
                <a href="http://www.cs.nthu.edu.tw/~lai/">Shang-Hong Lai</a>,
                <a href="https://sites.google.com/site/zjuwby/">Baoyuan Wang</a>
                <br>
                (*=equal contribution)
                <br>
                <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022 &nbsp
                <br>
                <a href="https://arxiv.org/abs/2203.14327">arXiv</a>
                /
                <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhu_Local-Adaptive_Face_Recognition_via_Graph-Based_Meta-Clustering_and_Regularized_Adaptation_CVPR_2022_paper.pdf">paper</a>
                /
                <a href="https://openaccess.thecvf.com/content/CVPR2022/supplemental/Zhu_Local-Adaptive_Face_Recognition_CVPR_2022_supplemental.pdf">supp</a>
                /
                <a href="https://youtu.be/Ng3GB46lNbo">video</a>
                <p></p>
                <p>
                  We introduce a new problem setup called "local-adaptive face recognition (LaFR)" and proposed the clustering and adaptation modules to address face recognition in unseen environments.
                </p>
            </td>
        </tr>
        
        
          
        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/fedfr.png" alt="blind-date" width="160" height="160">
                </td>
            <td width="75%" valign="middle">
                <papertitle>FedFR: Joint Optimization Federated Framework for Generic and Personalized Face Recognition</papertitle>
                
                <br>
                <a href="https://jackie840129.github.io/">Chih-Ting Liu*</a>,
                <strong>Chien-Yi Wang*</strong>, 
                <a href="http://media.ee.ntu.edu.tw/member/">Shao-Yi Chien</a>, 
                <a href="http://www.cs.nthu.edu.tw/~lai/">Shang-Hong Lai</a>
                &nbsp (*=equal contribution)
                <br>
                <em>36th AAAI Conference on Artificial Intelligence (AAAI)</em>, 2022 &nbsp <font color="red"><strong>(Oral)</strong></font>
                <br>
                <a href="https://arxiv.org/abs/2112.12496">arXiv</a>
                /
                <a href="https://github.com/jackie840129/FedFR">code (coming soon)</a> 
                <p></p>
                <p>
                  We propose a Federated Learning based framework called FedFR to improve the generic face representation in a privacy-aware manner. Besides, the framework jointly optimizes personalized models for the corresponding clients via the proposed Decoupled Feature Customization module.
                </p>
            </td>
        </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/wacv2022_pad.png" alt="blind-date" width="160" height="160">
                </td>
            <td width="75%" valign="middle">
                <papertitle>Disentangled Representation with Dual-stage Feature Learning for Face Anti-spoofing</papertitle>
                
                <br>
                <a href="mailto:yuchun@gapp.nthu.edu.tw">Yu-Chun Wang</a>,
                <strong>Chien-Yi Wang</strong>, 
                <a href="http://www.cs.nthu.edu.tw/~lai/">Shang-Hong Lai</a>
                <br>
                <em>IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em>, 2022 &nbsp
                <br>
                <a href="https://arxiv.org/abs/2110.09157">arXiv</a>
                /
                <a href="https://openaccess.thecvf.com/content/WACV2022/papers/Wang_Disentangled_Representation_With_Dual-Stage_Feature_Learning_for_Face_Anti-Spoofing_WACV_2022_paper.pdf">paper</a>
                /
                <a href="https://openaccess.thecvf.com/content/WACV2022/supplemental/Wang_Disentangled_Representation_With_WACV_2022_supplemental.pdf">supp</a>
                <p></p>
                <p>
                  We propose a novel dual-stage disentangled representation learning method that can efficiently untangle spoof-related features from irrelevant ones. Unlike previous FAS disentanglement works with one-stage architecture, we found that the dual-stage training design can improve the training stability and effectively encode the features to detect unseen attack types.
                </p>
            </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/fg2021_pad.png" alt="blind-date" width="160" height="160">
              </td>
          <td width="75%" valign="middle">
              <papertitle>High-Accuracy RGB-D Face Recognition via Segmentation-Aware Face Depth Estimation and Mask-Guided Attention Network</papertitle>
              
              <br>
              <a href="mailto:s108062802@m108.nthu.edu.tw">Meng-Tzu Chiu</a>,
              <a href="mailto:s107062523@m107.nthu.edu.tw">Hsun-Ying Cheng</a>,
              <strong>Chien-Yi Wang</strong>, 
              <a href="http://www.cs.nthu.edu.tw/~lai/">Shang-Hong Lai</a>
              <br>
              <em>IEEE International Conference on Automatic Face and Gesture Recognition (FG)</em>, 2021 &nbsp <font color="red"><strong>(Oral)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2112.11713">arXiv</a>
              /
              <a href="https://drive.google.com/file/d/1uC1r1hI2aHtmM5jmZSAyvR6iS4OfA3Pd/view?usp=sharing">poster</a>
              /
              <a href="https://drive.google.com/file/d/1nHbdb-B-viXfBlS0hBf4gE-SmWP9wp8n/view?usp=sharing">video</a>
              <p></p>
              <p>
                We propose to leverage pseudo facial segmentation and depth maps to assist the RGB-D face recognition task. With the multi-modal augmentation, the proposed mask-guided RGB-D recognition model could achieve superior performance on several benchmarks.
              </p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/bmvc2020_pad.png" alt="blind-date" width="160" height="160">
              </td>
          <td width="75%" valign="middle">
              <papertitle>Unified Representation Learning for Cross Model Compatibility</papertitle>
              
              <br>
              <strong>Chien-Yi Wang</strong>,
              <a href="https://amjltc295.github.io/">Ya-Liang Chang</a>,
              <a href="https://tw.linkedin.com/in/shang-ta-yang-852429133">Shang-Ta Yang</a>,
              <a href="http://www.dongchen.pro/">Dong Chen</a>,
              <a href="http://www.cs.nthu.edu.tw/~lai/">Shang-Hong Lai</a>
              <br>
              <em>British Machine Vision Conference (BMVC)</em>, 2020 &nbsp
              <br>
              <a href="https://arxiv.org/abs/2008.04821">arXiv</a>
              /
              <a href="https://www.bmvc2020-conference.com/assets/papers/0195.pdf">paper</a>
              <p></p>
              <p>
                We propose a unified representation learning framework to address the Cross Model Compatibility (CMC) problem in the context of visual search applications. The method can be applied onto face recognition and person re-identification tasks.
              </p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/iv2018_pad.png" alt="blind-date" width="160" height="160">
              </td>
          <td width="75%" valign="middle">
              <papertitle>A 3D Dynamic Scene Analysis Framework for Development of Intelligent Transportation Systems</papertitle>
              
              <br>
              <strong>Chien-Yi Wang</strong>, 
              <a href="https://scholar.google.com/citations?user=e3zULroAAAAJ&hl=en">Athma Narayanan</a>,
              <a href="https://www.linkedin.com/in/patilnabhi/">Abhishek Patil</a>,
              <a href="https://zhanwei.site/">Wei Zhan</a>,
              <a href="https://sites.google.com/site/yitingchen0524/home">Yi-Ting Chen</a>
              <br>
              <em>IEEE Intelligent Vehicles Symposium (IV)</em>, 2018 &nbsp
              <br>
              <a href="https://drive.google.com/file/d/1V1uANLvj-CQjCz57Y6idIRmRNQ-tP3Kk/view">paper</a>
              /
              <a href="https://drive.google.com/file/d/1gxqToHOL2_I_AHm1aXxKxyu0aApIT2fY/view?usp=sharing">video</a>
              <p></p>
              <p> 
                We propose a 3D dynamic scene analysis framework as the first step toward driving scene understanding. Specifically, given a sequence of synchronized 2D and 3D sensory data, the framework systematically integrates different perception modules to obtain 3D position, orientation, velocity and category of traffic participants and the ego car in a reconstructed 3D semantically labeled traffic scene.
              </p>
          </td>
        </tr>
        
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/iccv2015_pad.png" alt="blind-date" width="160" height="160">
              </td>
          <td width="75%" valign="middle">
              <papertitle>Robust Image Segmentation Using Contour-Guided Color Palette</papertitle>
              
              <br>
              <a href="https://scholar.google.com/citations?user=J7UFz6AAAAAJ&hl=en">Xiang Fu</a>,
              <strong>Chien-Yi Wang</strong>, 
              <a href="https://scholar.google.com/citations?user=4brexP0AAAAJ&hl=en">Chen Chen</a>,
              <a href="https://changhu.wang/">Changhu Wang</a>,
              <a href="http://mcl.usc.edu/people/cckuo/">C.-C. Jay Kuo</a>
              <br>
              <em>Proceedings of IEEE International Conference on Computer Vision (ICCV)</em>, 2015 &nbsp
              <br>
              <a href="https://openaccess.thecvf.com/content_iccv_2015/papers/Fu_Robust_Image_Segmentation_ICCV_2015_paper.pdf">paper</a>
              /
              <a href="https://github.com/fuxiang87/MCL_CCP">code</a>
              <p></p>
              <p> 
                The contour-guided color palette (CCP) is proposed for robust image segmentation. It efficiently integrates contour and color cues of an image.
              </p>
          </td>
        </tr>
        
        

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <heading>Service</heading>
              </td>
            </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
              <td width="75%" valign="center">
                <a href="https://2020.ieee-iv.org/">Reviewer, IV 2020</a>
                <br><br>              
                <a href="https://accv2020.github.io/">Reviewer, ACCV 2020</a>
                <br><br>
                <a href="https://www.bmvc2020-conference.com/">Reviewer, BMVC 2020</a>
                <br><br>
                <a href="https://www.bmvc2021-virtualconference.com/">Reviewer, BMVC 2021</a>
                <br><br>
                <a href="http://iccv2021.thecvf.com/home">Reviewer, ICCV 2021</a>
                <br><br>
                <a href="https://wacv2022.thecvf.com/home">Reviewer, WACV 2022</a>
                <br><br>
                <a href="https://cvpr2022.thecvf.com/">Reviewer, CVPR 2022</a>
              </td>
            </tr>
        </table>
        <br>
        <br>
        <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=tt&d=uty00DWjmG7UZ-8dcVXNUXb54olrrNY_jWSYn4Mi-2Q"></script>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                          Design and source code from <a style="font-size:small;" href="https://jonbarron.info">Jon Barron's website</a>
                  <br>
                  Last updated December 2021.
                      </p>
              </td>
            </tr>
        </table>
      </td>
    </tr>
  </table>
  
</body>

</html>
